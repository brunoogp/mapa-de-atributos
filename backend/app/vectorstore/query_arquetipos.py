from langchain.vectorstores import Chroma
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
import os
from dotenv import load_dotenv

# ğŸ” Carrega as variÃ¡veis de ambiente (.env com sua chave da OpenAI)
load_dotenv()

# ğŸ” Carrega a base vetorial existente
persist_directory = "app/vectorstore/index"

# Inicializa embeddings e base Chroma
embedding = OpenAIEmbeddings()
vectordb = Chroma(
    persist_directory=persist_directory,
    embedding_function=embedding
)

# Inicializa modelo de linguagem
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# Cria a chain de QA com recuperaÃ§Ã£o por similaridade
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectordb.as_retriever(),
    return_source_documents=True
)

# ğŸ“¥ Sua pergunta aqui
pergunta = "Qual arquÃ©tipo representa uma marca inovadora e rebelde?"

# ğŸ” Realiza a consulta
resposta = qa_chain(pergunta)

# âœ… Exibe a resposta
print("\nğŸ” Pergunta:", pergunta)
print("\nğŸ’¬ Resposta:", resposta['result'])

# ğŸ“ Fontes
print("\nğŸ“š Fontes:")
for doc in resposta["source_documents"]:
    print("-", doc.metadata.get("source", ""))
